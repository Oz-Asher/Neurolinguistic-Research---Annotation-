{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54830454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.55.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676773e",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92eeadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast, GPT2Model, GPT2Config\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT2WordClassifier:\n",
    "    def __init__(self, file_names, max_word_length=8, device=None, lr=1e-3, num_labels=5):\n",
    "        \"\"\"\n",
    "        file_names : list of CSV filenames (without .csv extension)\n",
    "        Each CSV must contain columns: 'word' and 'boundary' (0..num_labels-1)\n",
    "        \"\"\"\n",
    "        if isinstance(file_names, str):\n",
    "            file_names = [file_names]\n",
    "\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_word_length = max_word_length\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Load all data from the CSV files\n",
    "        self.texts, self.labels = self.get_data(file_names)\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", add_prefix_space=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Dataset & Dataloader\n",
    "        self.dataset = self.WordDataset(self.texts, self.labels, self.tokenizer, self.max_word_length)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "        # Model\n",
    "        self.model = self.GPT2ForTokenClassification(self.num_labels).to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "\n",
    "    # =========================\n",
    "    # DATA LOADING\n",
    "    # =========================\n",
    "    def get_data(self, file_names):\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for name in file_names:\n",
    "            file_path = os.path.join(os.getcwd(), f\"{name}.csv\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "                words = df['word'].to_list()\n",
    "                boundaries = df['boundary'].to_list()\n",
    "                # convert to int and clip to valid range\n",
    "                boundaries = [int(x) if str(x).isdigit() else 0 for x in boundaries]\n",
    "                boundaries = [min(max(0, x), self.num_labels - 1) for x in boundaries]\n",
    "          \n",
    "                texts.extend(words)\n",
    "                labels.extend(boundaries)\n",
    "            except FileNotFoundError:\n",
    "                print(f'File not found. Check if \"{name}.csv\" exists in {os.getcwd()}')\n",
    "                raise SystemExit\n",
    "        return texts, labels\n",
    "\n",
    "    # =========================\n",
    "    # DATASET\n",
    "    # =========================\n",
    "    class WordDataset(Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer, max_length):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            word = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "\n",
    "            encoding = self.tokenizer(\n",
    "                word,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=False  # avoid extra tokens\n",
    "            )\n",
    "\n",
    "            input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "            # label only the first token, rest = -100\n",
    "            token_labels = torch.full_like(input_ids, -100)\n",
    "            token_labels[0] = label\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": token_labels\n",
    "            }\n",
    "\n",
    "    # =========================\n",
    "    # MODEL\n",
    "    # =========================\n",
    "    class GPT2ForTokenClassification(nn.Module):\n",
    "        def __init__(self, num_labels=5):\n",
    "            super().__init__()\n",
    "            config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "            self.gpt2 = GPT2Model.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "            # Freeze GPT2\n",
    "            for param in self.gpt2.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "            outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = self.classifier(outputs.last_hidden_state)\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                # logits: [batch, seq_len, num_labels] â†’ CrossEntropy expects [batch, num_labels, seq_len]\n",
    "                loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "                loss = loss_fn(logits.permute(0, 2, 1), labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    # =========================\n",
    "    # TRAIN\n",
    "    # =========================\n",
    "    def train(self, epochs=50, early_stop_loss=0.1):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for batch in self.dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask, labels)\n",
    "                loss = outputs[\"loss\"]\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n",
    "            if loss.item() < early_stop_loss:\n",
    "                print(f\"Early stop at epoch {epoch+1}, loss={loss.item():.4f}\")\n",
    "                break\n",
    "\n",
    "    # =========================\n",
    "    # PREDICT\n",
    "    # =========================\n",
    "    def predict(self, sentence):\n",
    "        if isinstance(sentence, str):\n",
    "            words = sentence.split()\n",
    "        elif isinstance(sentence, list):\n",
    "            words = []\n",
    "            for item in sentence:\n",
    "                if isinstance(item, str) and \" \" in item:\n",
    "                    words.extend(item.split())\n",
    "                else:\n",
    "                    words.append(item)\n",
    "        else:\n",
    "            raise TypeError(\"sentence must be a string or a list\")\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=len(words)\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].to(self.device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids, attention_mask)[\"logits\"]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        word_ids = encoding.word_ids(0)\n",
    "        predictions = {}\n",
    "\n",
    "        for i, w_id in enumerate(word_ids):\n",
    "            if w_id is None:\n",
    "                continue\n",
    "            # store all probabilities\n",
    "            predictions[f\"{i} {words[i]}\"] = {f\"b{c}\": float(probs[0, i, c]) for c in range(self.num_labels)}\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe14eb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.0901\n",
      "Epoch 2 Loss: 0.0463\n",
      "Epoch 3 Loss: 0.6615\n"
     ]
    }
   ],
   "source": [
    "# List of CSV files without the .csv extension\n",
    "files = [\"sherlock_LLM\", \"merlin_LLM\"]\n",
    "\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = GPT2WordClassifier(files)\n",
    "\n",
    "# Train\n",
    "classifier.train(epochs=3, early_stop_loss=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090bbf4",
   "metadata": {},
   "source": [
    "## Boundary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca55e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sherlock, boundaries_sherlock = classifier.get_data(['sherlock_LLM'])\n",
    "words_merlin, boundaries_merlin = classifier.get_data(['merlin_LLM'])\n",
    "\n",
    "preds_sherlock1 = classifier.predict(words_sherlock[0:1000])\n",
    "preds_sherlock2 = classifier.predict(words_sherlock[1000:2000])\n",
    "preds_sherlock3 = classifier.predict(words_sherlock[2000:2697])\n",
    "\n",
    "preds_merlin1 = classifier.predict(words_merlin[0:1000])\n",
    "preds_merlin2 = classifier.predict(words_merlin[1000:2000])\n",
    "preds_merlin3 = classifier.predict(words_merlin[2000:2252])\n",
    "\n",
    "\n",
    "preds = {}\n",
    "current_index = 0\n",
    "\n",
    "for d in (preds_sherlock1, preds_sherlock2, preds_sherlock3, preds_merlin1, preds_merlin2, preds_merlin3):\n",
    "    for key, value in d.items():\n",
    "        word = key.split(\" \", 1)[1]   \n",
    "        preds[f\"{current_index} {word}\"] = value\n",
    "        current_index += 1\n",
    "\n",
    "\n",
    "# print(preds)\n",
    "\n",
    "boundaries = boundaries_sherlock + boundaries_merlin\n",
    "# print(boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75573ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean probability for 0 boundary: 0.8379610048527389\n",
      "Mean probability for 1 boundary: 0.1646124443388905\n",
      "Mean probability for 2 boundary: 0.014356996173809986\n",
      "Mean probability for 3 boundary: 0.00016643648040415746\n",
      "Mean probability for 4 boundary: 0.00019702845987170677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds_values = list(preds.values())\n",
    "\n",
    "for b in range(5):\n",
    "\n",
    "    indexes = [i for i, v in enumerate(boundaries) if v == b]\n",
    "    selected_preds = [preds_values[i] for i in indexes]\n",
    "    relevent_preds = [d[f\"b{b}\"] for d in selected_preds]\n",
    "\n",
    "    mean_preds = sum(relevent_preds) / len(relevent_preds)\n",
    "\n",
    "    \n",
    "    print(f'Mean probability for {b} boundary:', mean_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af531f1",
   "metadata": {},
   "source": [
    "# Magnifying probabilities (ignore here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2720b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m new_word_probs \u001b[38;5;241m=\u001b[39m {word: \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m word, value \u001b[38;5;129;01min\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_word_probs)\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'dict'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "new_word_probs = {word: 1 - math.exp(-value*100) for word, value in preds.items()}\n",
    "\n",
    "print(new_word_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean probability for 1 boundary after magnification: 0.5349009728593934\n",
      "mean probability for 0 boundary after magnification: 0.47111128639204386\n",
      "total probability after magnification 1.0060122592514373\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds_values = list(new_word_probs.values())\n",
    "\n",
    "\n",
    "indexes_of_ones = [i for i, v in enumerate(boundaries) if v == 1]\n",
    "selected_preds_1 = [preds_values[i] for i in indexes_of_ones]\n",
    "mean_ones = sum(selected_preds_1) / len(selected_preds_1)\n",
    "\n",
    "\n",
    "indexes_of_zero = [i for i, v in enumerate(boundaries) if v == 0]\n",
    "selected_preds_0 = [preds_values[i] for i in indexes_of_zero]\n",
    "mean_zeros = sum(selected_preds_0) / len(selected_preds_0)\n",
    "\n",
    "\n",
    "print('mean probability for 1 boundary after magnification:', mean_ones)\n",
    "print('mean probability for 0 boundary after magnification:', mean_zeros)\n",
    "print('total probability after magnification:', mean_ones + mean_zeros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
